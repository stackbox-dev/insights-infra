-- Create source tables (DDL for Kafka topics)
-- skus source table
CREATE TABLE `sbx-uat.encarta.public.skus` (
    id STRING,
    principal_id BIGINT,
    code STRING,
    name STRING,
    description STRING,
    short_description STRING,
    product_id STRING,
    ingredients STRING,
    shelf_life INT,
    fulfillment_type STRING,
    active_from TIMESTAMP(3),
    active_till TIMESTAMP(3),
    active BOOLEAN,
    identifier1 STRING,
    identifier2 STRING,
    invoice_life INT,
    avg_l0_per_put INT,
    inventory_type STRING,
    tag1 STRING,
    tag2 STRING,
    tag3 STRING,
    tag4 STRING,
    tag5 STRING,
    tag6 STRING,
    tag7 STRING,
    tag8 STRING,
    tag9 STRING,
    tag10 STRING,
    layers INT,
    cases_per_layer INT,
    handling_unit_type STRING,
    batch_date_print_level STRING,
    is_deleted BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.skus',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- products source table
CREATE TABLE `sbx-uat.encarta.public.products` (
    id STRING,
    principal_id BIGINT,
    code STRING,
    name STRING,
    description STRING,
    sub_category_id STRING,
    sub_brand_id STRING,
    dangerous BOOLEAN,
    spillable BOOLEAN,
    fragile BOOLEAN,
    flammable BOOLEAN,
    alcohol BOOLEAN,
    temperature_controlled BOOLEAN,
    temp_min DOUBLE,
    temp_max DOUBLE,
    cold_chain BOOLEAN,
    shelf_life INT,
    invoice_life INT,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.products',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- categories source table
CREATE TABLE `sbx-uat.encarta.public.categories` (
    id STRING,
    principal_id BIGINT,
    category_group_id STRING,
    code STRING,
    name STRING,
    description STRING,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.categories',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- sub_categories source table
CREATE TABLE `sbx-uat.encarta.public.sub_categories` (
    id STRING,
    principal_id BIGINT,
    category_id STRING,
    code STRING,
    name STRING,
    description STRING,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.sub_categories',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- category_groups source table
CREATE TABLE `sbx-uat.encarta.public.category_groups` (
    id STRING,
    principal_id BIGINT,
    code STRING,
    name STRING,
    description STRING,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.category_groups',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- brands source table
CREATE TABLE `sbx-uat.encarta.public.brands` (
    id STRING,
    principal_id BIGINT,
    code STRING,
    name STRING,
    description STRING,
    brand_owner_id STRING,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.brands',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- sub_brands source table
CREATE TABLE `sbx-uat.encarta.public.sub_brands` (
    id STRING,
    principal_id BIGINT,
    code STRING,
    name STRING,
    description STRING,
    brand_id STRING,
    active BOOLEAN,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.sub_brands',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- uoms source table
CREATE TABLE `sbx-uat.encarta.public.uoms` (
    id STRING,
    principal_id BIGINT,
    sku_id STRING,
    name STRING,
    hierarchy STRING,
    weight DOUBLE,
    volume DOUBLE,
    package_type STRING,
    length DOUBLE,
    width DOUBLE,
    height DOUBLE,
    units INT,
    packing_efficiency DOUBLE,
    active BOOLEAN,
    itf_code STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    erp_weight DOUBLE,
    erp_volume DOUBLE,
    erp_length DOUBLE,
    erp_width DOUBLE,
    erp_height DOUBLE,
    text_tag1 STRING,
    text_tag2 STRING,
    image STRING,
    num_tag1 DOUBLE,
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.uoms',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- classifications source table
CREATE TABLE `sbx-uat.encarta.public.classifications` (
    id STRING,
    principal_id BIGINT,
    sku_id STRING,
    type STRING,
    value STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.classifications',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- product_classifications source table
CREATE TABLE `sbx-uat.encarta.public.product_classifications` (
    id STRING,
    principal_id BIGINT,
    product_id STRING,
    type STRING,
    value STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    is_deleted BOOLEAN,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.product_classifications',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- skus_uoms_agg source table
CREATE TABLE `sbx-uat.encarta.public.skus_uoms_agg` (
    sku_id STRING,
    l0_units INT,
    l1_units INT,
    l2_units INT,
    l3_units INT,
    l0_name STRING,
    l0_weight DOUBLE,
    l0_volume DOUBLE,
    l0_package_type STRING,
    l0_length DOUBLE,
    l0_width DOUBLE,
    l0_height DOUBLE,
    l0_packing_efficiency DOUBLE,
    l0_itf_code STRING,
    l0_erp_weight DOUBLE,
    l0_erp_volume DOUBLE,
    l0_erp_length DOUBLE,
    l0_erp_width DOUBLE,
    l0_erp_height DOUBLE,
    l0_text_tag1 STRING,
    l0_text_tag2 STRING,
    l0_image STRING,
    l0_num_tag1 DOUBLE,
    l1_name STRING,
    l1_weight DOUBLE,
    l1_volume DOUBLE,
    l1_package_type STRING,
    l1_length DOUBLE,
    l1_width DOUBLE,
    l1_height DOUBLE,
    l1_packing_efficiency DOUBLE,
    l1_itf_code STRING,
    l1_erp_weight DOUBLE,
    l1_erp_volume DOUBLE,
    l1_erp_length DOUBLE,
    l1_erp_width DOUBLE,
    l1_erp_height DOUBLE,
    l1_text_tag1 STRING,
    l1_text_tag2 STRING,
    l1_image STRING,
    l1_num_tag1 DOUBLE,
    l2_name STRING,
    l2_weight DOUBLE,
    l2_volume DOUBLE,
    l2_package_type STRING,
    l2_length DOUBLE,
    l2_width DOUBLE,
    l2_height DOUBLE,
    l2_packing_efficiency DOUBLE,
    l2_itf_code STRING,
    l2_erp_weight DOUBLE,
    l2_erp_volume DOUBLE,
    l2_erp_length DOUBLE,
    l2_erp_width DOUBLE,
    l2_erp_height DOUBLE,
    l2_text_tag1 STRING,
    l2_text_tag2 STRING,
    l2_image STRING,
    l2_num_tag1 DOUBLE,
    l3_name STRING,
    l3_weight DOUBLE,
    l3_volume DOUBLE,
    l3_package_type STRING,
    l3_length DOUBLE,
    l3_width DOUBLE,
    l3_height DOUBLE,
    l3_packing_efficiency DOUBLE,
    l3_itf_code STRING,
    l3_erp_weight DOUBLE,
    l3_erp_volume DOUBLE,
    l3_erp_length DOUBLE,
    l3_erp_width DOUBLE,
    l3_erp_height DOUBLE,
    l3_text_tag1 STRING,
    l3_text_tag2 STRING,
    l3_image STRING,
    l3_num_tag1 DOUBLE,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (sku_id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.skus_uoms_agg',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- skus_classifications_agg source table
CREATE TABLE `sbx-uat.encarta.public.skus_classifications_agg` (
    sku_id STRING,
    classifications STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (sku_id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.skus_classifications_agg',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- products_classifications_agg source table
CREATE TABLE `sbx-uat.encarta.public.products_classifications_agg` (
    product_id STRING,
    product_classifications STRING,
    created_at TIMESTAMP(3),
    updated_at TIMESTAMP(3),
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (product_id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.products_classifications_agg',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);

-- Create final summary table structure (destination table)
CREATE TABLE `sbx-uat.encarta.public.skus_master` (
    id VARCHAR NOT NULL,
    principal_id BIGINT NOT NULL,
    node_id BIGINT NOT NULL,
    category VARCHAR,
    product VARCHAR,
    category_group VARCHAR,
    sub_brand VARCHAR,
    brand VARCHAR,
    code VARCHAR,
    name VARCHAR,
    short_description VARCHAR,
    description VARCHAR,
    fulfillment_type VARCHAR,
    avg_l0_per_put INT,
    inventory_type VARCHAR,
    shelf_life INT,
    identifier1 VARCHAR,
    identifier2 VARCHAR,
    tag1 VARCHAR,
    tag2 VARCHAR,
    tag3 VARCHAR,
    tag4 VARCHAR,
    tag5 VARCHAR,
    tag6 VARCHAR,
    tag7 VARCHAR,
    tag8 VARCHAR,
    tag9 VARCHAR,
    tag10 VARCHAR,
    handling_unit_type VARCHAR,
    cases_per_layer INT,
    layers INT,
    active_from TIMESTAMP(3),
    active_till TIMESTAMP(3),
    l0_units INT,
    l1_units INT,
    l2_units INT,
    l2_units_final INT,
    l3_units INT,
    l3_units_final INT,
    l0_name VARCHAR,
    l0_weight DOUBLE PRECISION,
    l0_volume DOUBLE PRECISION,
    l0_package_type VARCHAR,
    l0_length DOUBLE PRECISION,
    l0_width DOUBLE PRECISION,
    l0_height DOUBLE PRECISION,
    l0_packing_efficiency DOUBLE PRECISION,
    l0_itf_code VARCHAR,
    l0_erp_weight DOUBLE PRECISION,
    l0_erp_volume DOUBLE PRECISION,
    l0_erp_length DOUBLE PRECISION,
    l0_erp_width DOUBLE PRECISION,
    l0_erp_height DOUBLE PRECISION,
    l0_text_tag1 VARCHAR,
    l0_text_tag2 VARCHAR,
    l0_image VARCHAR,
    l0_num_tag1 DOUBLE PRECISION,
    l1_name VARCHAR,
    l1_weight DOUBLE PRECISION,
    l1_volume DOUBLE PRECISION,
    l1_package_type VARCHAR,
    l1_length DOUBLE PRECISION,
    l1_width DOUBLE PRECISION,
    l1_height DOUBLE PRECISION,
    l1_packing_efficiency DOUBLE PRECISION,
    l1_itf_code VARCHAR,
    l1_erp_weight DOUBLE PRECISION,
    l1_erp_volume DOUBLE PRECISION,
    l1_erp_length DOUBLE PRECISION,
    l1_erp_width DOUBLE PRECISION,
    l1_erp_height DOUBLE PRECISION,
    l1_text_tag1 VARCHAR,
    l1_text_tag2 VARCHAR,
    l1_image VARCHAR,
    l1_num_tag1 DOUBLE PRECISION,
    l2_name VARCHAR,
    l2_weight DOUBLE PRECISION,
    l2_volume DOUBLE PRECISION,
    l2_package_type VARCHAR,
    l2_length DOUBLE PRECISION,
    l2_width DOUBLE PRECISION,
    l2_height DOUBLE PRECISION,
    l2_packing_efficiency DOUBLE PRECISION,
    l2_itf_code VARCHAR,
    l2_erp_weight DOUBLE PRECISION,
    l2_erp_volume DOUBLE PRECISION,
    l2_erp_length DOUBLE PRECISION,
    l2_erp_width DOUBLE PRECISION,
    l2_erp_height DOUBLE PRECISION,
    l2_text_tag1 VARCHAR,
    l2_text_tag2 VARCHAR,
    l2_image VARCHAR,
    l2_num_tag1 DOUBLE PRECISION,
    l3_name VARCHAR,
    l3_weight DOUBLE PRECISION,
    l3_volume DOUBLE PRECISION,
    l3_package_type VARCHAR,
    l3_length DOUBLE PRECISION,
    l3_width DOUBLE PRECISION,
    l3_height DOUBLE PRECISION,
    l3_packing_efficiency DOUBLE PRECISION,
    l3_itf_code VARCHAR,
    l3_erp_weight DOUBLE PRECISION,
    l3_erp_volume DOUBLE PRECISION,
    l3_erp_length DOUBLE PRECISION,
    l3_erp_width DOUBLE PRECISION,
    l3_erp_height DOUBLE PRECISION,
    l3_text_tag1 VARCHAR,
    l3_text_tag2 VARCHAR,
    l3_image VARCHAR,
    l3_num_tag1 DOUBLE PRECISION,
    active BOOLEAN NOT NULL,
    classifications VARCHAR NOT NULL,
    product_classifications VARCHAR NOT NULL,
    is_deleted BOOLEAN NOT NULL,
    created_at TIMESTAMP(3) NOT NULL,
    updated_at TIMESTAMP(3) NOT NULL,
    proc_time AS PROCTIME(),
    event_time AS CASE
        WHEN updated_at > created_at THEN updated_at
        ELSE created_at
    END,
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND,
    PRIMARY KEY (id) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'sbx-uat.encarta.public.skus_master',
    'properties.bootstrap.servers' = 'bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092',
    'properties.security.protocol' = 'SASL_SSL',
    'properties.sasl.mechanism' = 'OAUTHBEARER',
    'properties.sasl.jaas.config' = 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required;',
    'properties.sasl.login.callback.handler.class' = 'com.google.cloud.hosted.kafka.auth.GcpLoginCallbackHandler',
    'properties.transaction.id.prefix' = 'encarta-skus-classifications-agg',
    'key.format' = 'avro-confluent',
    'key.avro-confluent.url' = 'http://cp-schema-registry.kafka',
    'value.format' = 'avro-confluent',
    'value.avro-confluent.url' = 'http://cp-schema-registry.kafka'
);
-- Continuously populate the summary table from source tables
-- Optimized INSERT statement using materialized aggregation tables
-- Uses regular LEFT JOINs for real-time latest data processing
-- Source 'skus' table has proper PRIMARY KEY (id) NOT ENFORCED with changelog.mode = 'upsert'
INSERT INTO `sbx-uat.encarta.public.skus_master`
SELECT s.id,
    -- Primary key from source matches target primary key
    s.principal_id,
    s.principal_id AS node_id,
    cat.code AS category,
    p.code AS product,
    cg.code AS category_group,
    sb.code AS sub_brand,
    b.code AS brand,
    s.code,
    s.name,
    s.short_description,
    s.description,
    s.fulfillment_type,
    s.avg_l0_per_put,
    s.inventory_type,
    s.shelf_life,
    s.identifier1,
    s.identifier2,
    s.tag1,
    s.tag2,
    s.tag3,
    s.tag4,
    s.tag5,
    s.tag6,
    s.tag7,
    s.tag8,
    s.tag9,
    s.tag10,
    s.handling_unit_type,
    s.cases_per_layer,
    s.layers,
    s.active_from,
    s.active_till,
    COALESCE(uom_agg.l0_units, 0) AS l0_units,
    COALESCE(uom_agg.l1_units, 0) AS l1_units,
    COALESCE(uom_agg.l2_units, 0) AS l2_units,
    COALESCE(uom_agg.l2_units, 0) AS l2_units_final,
    COALESCE(uom_agg.l3_units, 0) AS l3_units,
    COALESCE(uom_agg.l3_units, 0) AS l3_units_final,
    uom_agg.l0_name,
    uom_agg.l0_weight,
    uom_agg.l0_volume,
    uom_agg.l0_package_type,
    uom_agg.l0_length,
    uom_agg.l0_width,
    uom_agg.l0_height,
    uom_agg.l0_packing_efficiency,
    uom_agg.l0_itf_code,
    uom_agg.l0_erp_weight,
    uom_agg.l0_erp_volume,
    uom_agg.l0_erp_length,
    uom_agg.l0_erp_width,
    uom_agg.l0_erp_height,
    uom_agg.l0_text_tag1,
    uom_agg.l0_text_tag2,
    uom_agg.l0_image,
    uom_agg.l0_num_tag1,
    uom_agg.l1_name,
    uom_agg.l1_weight,
    uom_agg.l1_volume,
    uom_agg.l1_package_type,
    uom_agg.l1_length,
    uom_agg.l1_width,
    uom_agg.l1_height,
    uom_agg.l1_packing_efficiency,
    uom_agg.l1_itf_code,
    uom_agg.l1_erp_weight,
    uom_agg.l1_erp_volume,
    uom_agg.l1_erp_length,
    uom_agg.l1_erp_width,
    uom_agg.l1_erp_height,
    uom_agg.l1_text_tag1,
    uom_agg.l1_text_tag2,
    uom_agg.l1_image,
    uom_agg.l1_num_tag1,
    uom_agg.l2_name,
    uom_agg.l2_weight,
    uom_agg.l2_volume,
    uom_agg.l2_package_type,
    uom_agg.l2_length,
    uom_agg.l2_width,
    uom_agg.l2_height,
    uom_agg.l2_packing_efficiency,
    uom_agg.l2_itf_code,
    uom_agg.l2_erp_weight,
    uom_agg.l2_erp_volume,
    uom_agg.l2_erp_length,
    uom_agg.l2_erp_width,
    uom_agg.l2_erp_height,
    uom_agg.l2_text_tag1,
    uom_agg.l2_text_tag2,
    uom_agg.l2_image,
    uom_agg.l2_num_tag1,
    uom_agg.l3_name,
    uom_agg.l3_weight,
    uom_agg.l3_volume,
    uom_agg.l3_package_type,
    uom_agg.l3_length,
    uom_agg.l3_width,
    uom_agg.l3_height,
    uom_agg.l3_packing_efficiency,
    uom_agg.l3_itf_code,
    uom_agg.l3_erp_weight,
    uom_agg.l3_erp_volume,
    uom_agg.l3_erp_length,
    uom_agg.l3_erp_width,
    uom_agg.l3_erp_height,
    uom_agg.l3_text_tag1,
    uom_agg.l3_text_tag2,
    uom_agg.l3_image,
    uom_agg.l3_num_tag1,
    COALESCE(s.active, FALSE) AS active,
    COALESCE(class_agg.classifications, '{}') AS classifications,
    COALESCE(prod_class_agg.product_classifications, '{}') AS product_classifications,
    COALESCE(s.is_deleted, FALSE) AS is_deleted,
    s.created_at,
    s.updated_at
FROM `sbx-uat.encarta.public.skus` s
    LEFT JOIN `sbx-uat.encarta.public.skus_uoms_agg` AS uom_agg ON s.id = uom_agg.sku_id
    LEFT JOIN `sbx-uat.encarta.public.skus_classifications_agg` AS class_agg ON s.id = class_agg.sku_id
    LEFT JOIN `sbx-uat.encarta.public.products_classifications_agg` AS prod_class_agg ON s.product_id = prod_class_agg.product_id
    LEFT JOIN `sbx-uat.encarta.public.products` AS p ON s.product_id = p.id
    LEFT JOIN `sbx-uat.encarta.public.sub_categories` AS subcat ON p.sub_category_id = subcat.id
    LEFT JOIN `sbx-uat.encarta.public.categories` AS cat ON subcat.category_id = cat.id
    LEFT JOIN `sbx-uat.encarta.public.category_groups` AS cg ON cat.category_group_id = cg.id
    LEFT JOIN `sbx-uat.encarta.public.sub_brands` AS sb ON p.sub_brand_id = sb.id
    LEFT JOIN `sbx-uat.encarta.public.brands` AS b ON sb.brand_id = b.id;