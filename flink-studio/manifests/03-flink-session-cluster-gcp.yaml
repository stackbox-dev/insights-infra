apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: flink-session-cluster
  namespace: flink-studio
  labels:
    app.kubernetes.io/name: flink-session-cluster
    app.kubernetes.io/component: flink-cluster
    app.kubernetes.io/cloud: gcp
spec:
  image: flink:2.0.0-scala_2.12-java21
  flinkVersion: v2_0
  imagePullPolicy: IfNotPresent
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "4"
    state.backend: "hashmap"
    # Force session cluster to start TaskManager pods immediately
    slotmanager.number-of-slots.min: "8"
    # GCS paths for Flink state management
    state.checkpoints.dir: gs://sbx-stag-flink-storage/checkpoints
    state.savepoints.dir: gs://sbx-stag-flink-storage/savepoints
    high-availability.type: kubernetes
    high-availability.storageDir: gs://sbx-stag-flink-storage/ha
    restart-strategy.type: failure-rate
    restart-strategy.failure-rate.max-failures-per-interval: "10"
    restart-strategy.failure-rate.failure-rate-interval: "10 min"
    restart-strategy.failure-rate.delay: "30 s"
    execution.checkpointing.interval: "30 s"
    execution.checkpointing.mode: EXACTLY_ONCE
    execution.checkpointing.timeout: "10 min"
    table.exec.source.idle-timeout: "30 s"
    # Hadoop configuration for GCS
    fs.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
    fs.AbstractFileSystem.gs.impl: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
    fs.gs.project.id: sbx-stag
    fs.gs.auth.service.account.enable: "true"
    fs.gs.auth.service.account.json.keyfile: /opt/flink/gcp/service-account.json
    # Kafka configuration for GCP managed Kafka
    kafka.bootstrap.servers: "bootstrap.sbx-kafka-cluster.asia-south1.managedkafka.sbx-stag.cloud.goog:9092"
    kafka.security.protocol: "SASL_SSL"
    kafka.sasl.mechanism: "OAUTHBEARER"
    kafka.sasl.jaas.config: 'org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="unused" clientSecret="unused";'
    kafka.sasl.login.callback.handler.class: "org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginCallbackHandler"
    kafka.sasl.oauthbearer.token.endpoint.url: "http://localhost:14293"
  serviceAccount: flink
  jobManager:
    resource:
      memory: "1536m"
      cpu: 1
    replicas: 1
    podTemplate:
      spec:
        serviceAccountName: flink
        initContainers:
          - name: flink-plugin-setup
            image: flink:2.0.0-scala_2.12-java21
            command:
              - "sh"
              - "-c"
              - |
                echo "Loading all Flink connectors and plugins..."
                mkdir -p /mnt/plugins/gs-fs-hadoop
                mkdir -p /mnt/plugins/s3-fs-hadoop  
                mkdir -p /mnt/plugins/s3-fs-presto
                mkdir -p /mnt/plugins/azure-fs-hadoop
                mkdir -p /mnt/plugins/oss-fs-hadoop

                # Copy all filesystem connectors
                cp /opt/flink/opt/flink-gs-fs-hadoop-*.jar /mnt/plugins/gs-fs-hadoop/ 2>/dev/null || echo "GCS connector not found"
                cp /opt/flink/opt/flink-s3-fs-hadoop-*.jar /mnt/plugins/s3-fs-hadoop/ 2>/dev/null || echo "S3 Hadoop connector not found"  
                cp /opt/flink/opt/flink-s3-fs-presto-*.jar /mnt/plugins/s3-fs-presto/ 2>/dev/null || echo "S3 Presto connector not found"
                cp /opt/flink/opt/flink-azure-fs-hadoop-*.jar /mnt/plugins/azure-fs-hadoop/ 2>/dev/null || echo "Azure connector not found"
                cp /opt/flink/opt/flink-oss-fs-hadoop-*.jar /mnt/plugins/oss-fs-hadoop/ 2>/dev/null || echo "OSS connector not found"

                # Copy Kafka connector to lib directory (not plugins)
                echo "Setting up Kafka connector..."
                # For Flink 2.0, we need to download the Kafka connector
                wget -O /tmp/flink-sql-connector-kafka-2.0.0.jar https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/4.0.0-2.0/flink-sql-connector-kafka-4.0.0-2.0.jar || echo "Kafka connector download failed"
                # Copy to the actual Flink lib directory (not mounting over it)
                cp /tmp/flink-sql-connector-kafka-2.0.0.jar /opt/flink/lib/ || echo "Failed to copy Kafka connector to lib"

                echo "Available plugins in /opt/flink/opt/:"
                ls -la /opt/flink/opt/
                echo "Enabled plugins:"
                find /mnt/plugins -name "*.jar" -exec echo "  {}" \;
                echo "Lib directory contents:"
                ls -la /opt/flink/lib/
            volumeMounts:
              - name: flink-plugins
                mountPath: /mnt/plugins
        containers:
          - name: flink-main-container
            env:
              - name: HADOOP_CONF_DIR
                value: /opt/hadoop/conf
              - name: GOOGLE_APPLICATION_CREDENTIALS
                value: /opt/flink/gcp/service-account.json
              - name: GOOGLE_CLOUD_PROJECT
                value: sbx-stag
            volumeMounts:
              - name: flink-plugins
                mountPath: /opt/flink/plugins
              - name: gcp-service-account
                mountPath: /opt/flink/gcp
                readOnly: true
              - name: hadoop-config
                mountPath: /opt/hadoop/conf
                readOnly: true
        volumes:
          - name: flink-plugins
            emptyDir: {}
          - name: gcp-service-account
            secret:
              secretName: gcp-service-account-key
          - name: hadoop-config
            configMap:
              name: hadoop-config
  taskManager:
    resource:
      memory: "3072m"
      cpu: 1
    replicas: 2
    podTemplate:
      spec:
        serviceAccountName: flink
        initContainers:
          - name: flink-plugin-setup
            image: flink:2.0.0-scala_2.12-java21
            command:
              - "sh"
              - "-c"
              - |
                echo "Loading all Flink connectors and plugins..."
                mkdir -p /mnt/plugins/gs-fs-hadoop
                mkdir -p /mnt/plugins/s3-fs-hadoop  
                mkdir -p /mnt/plugins/s3-fs-presto
                mkdir -p /mnt/plugins/azure-fs-hadoop
                mkdir -p /mnt/plugins/oss-fs-hadoop

                # Copy all filesystem connectors
                cp /opt/flink/opt/flink-gs-fs-hadoop-*.jar /mnt/plugins/gs-fs-hadoop/ 2>/dev/null || echo "GCS connector not found"
                cp /opt/flink/opt/flink-s3-fs-hadoop-*.jar /mnt/plugins/s3-fs-hadoop/ 2>/dev/null || echo "S3 Hadoop connector not found"  
                cp /opt/flink/opt/flink-s3-fs-presto-*.jar /mnt/plugins/s3-fs-presto/ 2>/dev/null || echo "S3 Presto connector not found"
                cp /opt/flink/opt/flink-azure-fs-hadoop-*.jar /mnt/plugins/azure-fs-hadoop/ 2>/dev/null || echo "Azure connector not found"
                cp /opt/flink/opt/flink-oss-fs-hadoop-*.jar /mnt/plugins/oss-fs-hadoop/ 2>/dev/null || echo "OSS connector not found"

                # Copy Kafka connector to lib directory (not plugins)
                echo "Setting up Kafka connector..."
                # For Flink 2.0, we need to download the Kafka connector
                wget -O /tmp/flink-sql-connector-kafka-2.0.0.jar https://repo1.maven.org/maven2/org/apache/flink/flink-sql-connector-kafka/4.0.0-2.0/flink-sql-connector-kafka-4.0.0-2.0.jar || echo "Kafka connector download failed"
                # Copy to the actual Flink lib directory (not mounting over it)
                cp /tmp/flink-sql-connector-kafka-2.0.0.jar /opt/flink/lib/ || echo "Failed to copy Kafka connector to lib"

                echo "Available plugins in /opt/flink/opt/:"
                ls -la /opt/flink/opt/
                echo "Enabled plugins:"
                find /mnt/plugins -name "*.jar" -exec echo "  {}" \;
                echo "Lib directory contents:"
                ls -la /opt/flink/lib/
            volumeMounts:
              - name: flink-plugins
                mountPath: /mnt/plugins
        containers:
          - name: flink-main-container
            env:
              - name: HADOOP_CONF_DIR
                value: /opt/hadoop/conf
              - name: GOOGLE_APPLICATION_CREDENTIALS
                value: /opt/flink/gcp/service-account.json
              - name: GOOGLE_CLOUD_PROJECT
                value: sbx-stag
            volumeMounts:
              - name: flink-plugins
                mountPath: /opt/flink/plugins
              - name: gcp-service-account
                mountPath: /opt/flink/gcp
                readOnly: true
              - name: hadoop-config
                mountPath: /opt/hadoop/conf
                readOnly: true
        volumes:
          - name: flink-plugins
            emptyDir: {}
          - name: gcp-service-account
            secret:
              secretName: gcp-service-account-key
          - name: hadoop-config
            configMap:
              name: hadoop-config
  mode: native
